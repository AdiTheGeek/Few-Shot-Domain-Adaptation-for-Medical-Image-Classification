{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8f45167",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccaeeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58440300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "ram_gb = psutil.virtual_memory().total / 1e9\n",
    "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository (or upload files)\n",
    "!git clone https://github.com/YourRepo/Few-Shot-Domain-Adaptation-for-Medical-Image-Classification.git\n",
    "%cd Few-Shot-Domain-Adaptation-for-Medical-Image-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision timm transformers scikit-learn pandas Pillow matplotlib pytorch-lightning wandb opencv-python scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499ca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Add project to path (if needed)\n",
    "# sys.path.append('/content/Few-Shot-Domain-Adaptation-for-Medical-Image-Classification')\n",
    "\n",
    "from configs.config import Config\n",
    "from data.datasets import SimpleMedicalDataset, get_transforms, sample_few_shot_indices\n",
    "from models.vit_backbone import ViTWrapper\n",
    "from models.cnn_backbones import build_cnn\n",
    "from lora.lora import apply_lora_to_model\n",
    "from adapters.adapter import attach_adapter_to_vit\n",
    "from prompts.prompt_tuning import attach_visual_prompt_to_vit\n",
    "from train.trainer import LitModel\n",
    "from eval.evaluator import compute_metrics, bootstrap_confidence_interval\n",
    "from utils.utils import set_seed, count_parameters\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb004bc7",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation\n",
    "\n",
    "**Note:** You need to prepare your CheXpert and NIH ChestX-ray14 datasets.\n",
    "\n",
    "Expected structure:\n",
    "```\n",
    "data/\n",
    "â”œâ”€â”€ chexpert_train.csv\n",
    "â”œâ”€â”€ chexpert_val.csv\n",
    "â”œâ”€â”€ nih_train.csv\n",
    "â”œâ”€â”€ nih_val.csv\n",
    "â”œâ”€â”€ images/\n",
    "â”‚   â”œâ”€â”€ patient001/\n",
    "â”‚   â”‚   â”œâ”€â”€ study1/\n",
    "â”‚   â”‚   â”‚   â””â”€â”€ view1.jpg\n",
    "```\n",
    "\n",
    "CSV format:\n",
    "- Column `Path`: relative image path\n",
    "- Columns for each of 14 pathologies (0/1 labels, -1 for uncertainty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb0c287",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740d7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download ashery/chexpert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee065e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip air-heart-disease.zip -d ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ec03cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths (modify as needed)\n",
    "DATA_ROOT = './data'\n",
    "CHEXPERT_TRAIN = f'{DATA_ROOT}/chexpert_train.csv'\n",
    "CHEXPERT_VAL = f'{DATA_ROOT}/chexpert_val.csv'\n",
    "NIH_TRAIN = f'{DATA_ROOT}/nih_train.csv'\n",
    "NIH_VAL = f'{DATA_ROOT}/nih_val.csv'\n",
    "NIH_TEST = f'{DATA_ROOT}/nih_test.csv'\n",
    "\n",
    "# Create checkpoint directory\n",
    "!mkdir -p checkpoints logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify datasets\n",
    "print(\"Checking dataset files...\")\n",
    "for csv_path in [CHEXPERT_TRAIN, CHEXPERT_VAL, NIH_TRAIN, NIH_VAL]:\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"âœ“ {csv_path}: {len(df)} samples\")\n",
    "    else:\n",
    "        print(f\"âœ— {csv_path}: NOT FOUND\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079506a6",
   "metadata": {},
   "source": [
    "## 3. Configuration and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87730115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base configuration\n",
    "config = Config(\n",
    "    data_root=DATA_ROOT,\n",
    "    img_size=224,\n",
    "    num_classes=14,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    backbone='vit_base_patch16_224',\n",
    "    pretrained=True,\n",
    "    epochs=30,\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4,\n",
    "    optimizer='adamw',\n",
    "    mixed_precision=True,\n",
    "    gradient_checkpointing=True,\n",
    "    few_shot_k=50,\n",
    "    checkpoint_dir='./checkpoints',\n",
    "    use_wandb=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "set_seed(config.seed)\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Backbone: {config.backbone}\")\n",
    "print(f\"  Batch size: {config.batch_size}\")\n",
    "print(f\"  Epochs: {config.epochs}\")\n",
    "print(f\"  Learning rate: {config.lr}\")\n",
    "print(f\"  Mixed precision: {config.mixed_precision}\")\n",
    "print(f\"  Gradient checkpointing: {config.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ad932",
   "metadata": {},
   "source": [
    "## 4. Experiment 1: Baseline ViT on Source Domain (CheXpert)\n",
    "\n",
    "Train a standard Vision Transformer on the source domain without any adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af615354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CheXpert datasets\n",
    "train_ds = SimpleMedicalDataset(CHEXPERT_TRAIN, DATA_ROOT, transform=get_transforms(config.img_size, True))\n",
    "val_ds = SimpleMedicalDataset(CHEXPERT_VAL, DATA_ROOT, transform=get_transforms(config.img_size, False))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=config.batch_size, shuffle=True, \n",
    "                                           num_workers=config.num_workers, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=config.batch_size, shuffle=False, \n",
    "                                         num_workers=config.num_workers, pin_memory=True)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497ad0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline ViT model\n",
    "baseline_model = ViTWrapper(model_name=config.backbone, num_classes=config.num_classes, pretrained=config.pretrained)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "if hasattr(baseline_model.backbone, 'set_grad_checkpointing'):\n",
    "    baseline_model.backbone.set_grad_checkpointing(True)\n",
    "\n",
    "params = count_parameters(baseline_model)\n",
    "print(f\"Total parameters: {params['total']:,}\")\n",
    "print(f\"Trainable parameters: {params['trainable']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a59a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Lightning trainer\n",
    "lit_baseline = LitModel(baseline_model, config)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='./checkpoints/baseline_vit',\n",
    "    filename='best-{epoch:02d}-{val/auc:.4f}',\n",
    "    monitor='val/auc',\n",
    "    mode='max',\n",
    "    save_top_k=3\n",
    ")\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val/auc', patience=10, mode='max', verbose=True)\n",
    "\n",
    "trainer_baseline = pl.Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator='gpu',\n",
    "    devices=-1,  # Use all available GPUs\n",
    "    precision=16 if config.mixed_precision else 32,\n",
    "    callbacks=[checkpoint_callback, early_stop],\n",
    "    gradient_clip_val=1.0,\n",
    "    log_every_n_steps=10\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training baseline ViT on CheXpert...\")\n",
    "trainer_baseline.fit(lit_baseline, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b59904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save baseline checkpoint\n",
    "torch.save({\n",
    "    'model_state_dict': baseline_model.state_dict(),\n",
    "    'config': config\n",
    "}, './checkpoints/baseline_vit_source.pth')\n",
    "\n",
    "print(f\"âœ“ Baseline model saved\")\n",
    "print(f\"Best val AUC: {checkpoint_callback.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189339e3",
   "metadata": {},
   "source": [
    "## 5. Experiment 2: Few-Shot LoRA Adaptation (CheXpert â†’ NIH)\n",
    "\n",
    "Apply LoRA to adapt the baseline model to NIH dataset with limited samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bb751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NIH target dataset\n",
    "nih_train_full = SimpleMedicalDataset(NIH_TRAIN, DATA_ROOT, transform=get_transforms(config.img_size, True))\n",
    "nih_val = SimpleMedicalDataset(NIH_VAL, DATA_ROOT, transform=get_transforms(config.img_size, False))\n",
    "\n",
    "# Sample few-shot subset\n",
    "few_shot_indices = sample_few_shot_indices(nih_train_full, k_per_class=config.few_shot_k, seed=config.seed)\n",
    "print(f\"Few-shot sampling: {len(few_shot_indices)} samples (k={config.few_shot_k} per class)\")\n",
    "\n",
    "nih_train_fewshot = Subset(nih_train_full, few_shot_indices)\n",
    "\n",
    "train_loader_nih = torch.utils.data.DataLoader(nih_train_fewshot, batch_size=config.batch_size, \n",
    "                                                shuffle=True, num_workers=config.num_workers, pin_memory=True)\n",
    "val_loader_nih = torch.utils.data.DataLoader(nih_val, batch_size=config.batch_size, \n",
    "                                              shuffle=False, num_workers=config.num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d25a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline checkpoint and apply LoRA\n",
    "lora_model = ViTWrapper(model_name=config.backbone, num_classes=config.num_classes, pretrained=False)\n",
    "checkpoint = torch.load('./checkpoints/baseline_vit_source.pth')\n",
    "lora_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"âœ“ Loaded baseline checkpoint\")\n",
    "\n",
    "# Apply LoRA\n",
    "apply_lora_to_model(lora_model, r=8, alpha=32.0)\n",
    "\n",
    "# Freeze original parameters\n",
    "for name, param in lora_model.named_parameters():\n",
    "    if 'lora_' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "lora_params = count_parameters(lora_model)\n",
    "print(f\"\\nLoRA model parameters:\")\n",
    "print(f\"  Total: {lora_params['total']:,}\")\n",
    "print(f\"  Trainable: {lora_params['trainable']:,}\")\n",
    "print(f\"  Efficiency: {100.0 * lora_params['trainable'] / lora_params['total']:.2f}% trainable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfd679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with LoRA\n",
    "lit_lora = LitModel(lora_model, config)\n",
    "\n",
    "checkpoint_callback_lora = ModelCheckpoint(\n",
    "    dirpath='./checkpoints/lora_adaptation',\n",
    "    filename='best-{epoch:02d}-{val/auc:.4f}',\n",
    "    monitor='val/auc',\n",
    "    mode='max',\n",
    "    save_top_k=3\n",
    ")\n",
    "\n",
    "trainer_lora = pl.Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator='gpu',\n",
    "    devices=-1,\n",
    "    precision=16 if config.mixed_precision else 32,\n",
    "    callbacks=[checkpoint_callback_lora, EarlyStopping(monitor='val/auc', patience=10, mode='max')],\n",
    "    gradient_clip_val=1.0\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training with LoRA adaptation...\")\n",
    "trainer_lora.fit(lit_lora, train_dataloaders=train_loader_nih, val_dataloaders=val_loader_nih)\n",
    "\n",
    "print(f\"âœ“ LoRA training complete\")\n",
    "print(f\"Best val AUC: {checkpoint_callback_lora.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0f10d1",
   "metadata": {},
   "source": [
    "## 6. Experiment 3: Few-Shot Adapter Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc47fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline and apply adapters\n",
    "adapter_model = ViTWrapper(model_name=config.backbone, num_classes=config.num_classes, pretrained=False)\n",
    "checkpoint = torch.load('./checkpoints/baseline_vit_source.pth')\n",
    "adapter_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Attach adapters\n",
    "attach_adapter_to_vit(adapter_model.backbone, adapter_dim=64)\n",
    "\n",
    "# Freeze backbone\n",
    "for name, param in adapter_model.named_parameters():\n",
    "    if 'adapter' not in name and 'classifier' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "adapter_params = count_parameters(adapter_model)\n",
    "print(f\"Adapter model - Trainable: {adapter_params['trainable']:,} ({100.0 * adapter_params['trainable'] / adapter_params['total']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfcafdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with adapters\n",
    "lit_adapter = LitModel(adapter_model, config)\n",
    "\n",
    "checkpoint_callback_adapter = ModelCheckpoint(\n",
    "    dirpath='./checkpoints/adapter_adaptation',\n",
    "    filename='best-{epoch:02d}-{val/auc:.4f}',\n",
    "    monitor='val/auc',\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "trainer_adapter = pl.Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator='gpu',\n",
    "    devices=-1,\n",
    "    precision=16,\n",
    "    callbacks=[checkpoint_callback_adapter, EarlyStopping(monitor='val/auc', patience=10, mode='max')]\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training with Adapter adaptation...\")\n",
    "trainer_adapter.fit(lit_adapter, train_dataloaders=train_loader_nih, val_dataloaders=val_loader_nih)\n",
    "print(f\"Best val AUC: {checkpoint_callback_adapter.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac8dcbd",
   "metadata": {},
   "source": [
    "## 7. Experiment 4: Few-Shot Prompt Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f9e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline and apply prompt tuning\n",
    "prompt_model = ViTWrapper(model_name=config.backbone, num_classes=config.num_classes, pretrained=False)\n",
    "checkpoint = torch.load('./checkpoints/baseline_vit_source.pth')\n",
    "prompt_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Attach visual prompts\n",
    "attach_visual_prompt_to_vit(prompt_model.backbone, prompt_tokens=10)\n",
    "\n",
    "# Freeze everything except prompts and classifier\n",
    "for name, param in prompt_model.named_parameters():\n",
    "    if 'visual_prompt' not in name and 'classifier' not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "prompt_params = count_parameters(prompt_model)\n",
    "print(f\"Prompt model - Trainable: {prompt_params['trainable']:,} ({100.0 * prompt_params['trainable'] / prompt_params['total']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9569994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with prompt tuning\n",
    "lit_prompt = LitModel(prompt_model, config)\n",
    "\n",
    "checkpoint_callback_prompt = ModelCheckpoint(\n",
    "    dirpath='./checkpoints/prompt_adaptation',\n",
    "    filename='best-{epoch:02d}-{val/auc:.4f}',\n",
    "    monitor='val/auc',\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "trainer_prompt = pl.Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator='gpu',\n",
    "    devices=-1,\n",
    "    precision=16,\n",
    "    callbacks=[checkpoint_callback_prompt, EarlyStopping(monitor='val/auc', patience=10, mode='max')]\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training with Prompt Tuning...\")\n",
    "trainer_prompt.fit(lit_prompt, train_dataloaders=train_loader_nih, val_dataloaders=val_loader_nih)\n",
    "print(f\"Best val AUC: {checkpoint_callback_prompt.best_model_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3cde4a",
   "metadata": {},
   "source": [
    "## 8. Experiment 5: CNN Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e522adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet-50 baseline\n",
    "resnet_model = build_cnn('resnet50', num_classes=config.num_classes, pretrained=True)\n",
    "lit_resnet = LitModel(resnet_model, config)\n",
    "\n",
    "trainer_resnet = pl.Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator='gpu',\n",
    "    devices=-1,\n",
    "    precision=16,\n",
    "    callbacks=[ModelCheckpoint(dirpath='./checkpoints/resnet50', monitor='val/auc', mode='max')]\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training ResNet-50 baseline...\")\n",
    "trainer_resnet.fit(lit_resnet, train_dataloaders=train_loader_nih, val_dataloaders=val_loader_nih)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9451bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DenseNet-121 baseline\n",
    "densenet_model = build_cnn('densenet121', num_classes=config.num_classes, pretrained=True)\n",
    "lit_densenet = LitModel(densenet_model, config)\n",
    "\n",
    "trainer_densenet = pl.Trainer(\n",
    "    max_epochs=config.epochs,\n",
    "    accelerator='gpu',\n",
    "    devices=-1,\n",
    "    precision=16,\n",
    "    callbacks=[ModelCheckpoint(dirpath='./checkpoints/densenet121', monitor='val/auc', mode='max')]\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Training DenseNet-121 baseline...\")\n",
    "trainer_densenet.fit(lit_densenet, train_dataloaders=train_loader_nih, val_dataloaders=val_loader_nih)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309f006",
   "metadata": {},
   "source": [
    "## 9. Results Comparison and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results\n",
    "results = {\n",
    "    'Baseline ViT': checkpoint_callback.best_model_score.item(),\n",
    "    'LoRA': checkpoint_callback_lora.best_model_score.item(),\n",
    "    'Adapter': checkpoint_callback_adapter.best_model_score.item(),\n",
    "    'Prompt Tuning': checkpoint_callback_prompt.best_model_score.item(),\n",
    "}\n",
    "\n",
    "# Parameter efficiency\n",
    "param_counts = {\n",
    "    'Baseline ViT': params['trainable'],\n",
    "    'LoRA': lora_params['trainable'],\n",
    "    'Adapter': adapter_params['trainable'],\n",
    "    'Prompt Tuning': prompt_params['trainable'],\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Method': list(results.keys()),\n",
    "    'Val AUC': list(results.values()),\n",
    "    'Trainable Params': list(param_counts.values())\n",
    "})\n",
    "\n",
    "comparison_df['Param Efficiency (%)'] = 100.0 * comparison_df['Trainable Params'] / params['total']\n",
    "comparison_df = comparison_df.sort_values('Val AUC', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5978d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# AUC comparison\n",
    "axes[0].bar(comparison_df['Method'], comparison_df['Val AUC'], color=['blue', 'green', 'orange', 'red'])\n",
    "axes[0].set_ylabel('Validation AUC', fontsize=12)\n",
    "axes[0].set_title('Performance Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Parameter efficiency\n",
    "axes[1].bar(comparison_df['Method'], comparison_df['Param Efficiency (%)'], color=['blue', 'green', 'orange', 'red'])\n",
    "axes[1].set_ylabel('Trainable Parameters (%)', fontsize=12)\n",
    "axes[1].set_title('Parameter Efficiency', fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Results saved to results_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b4411",
   "metadata": {},
   "source": [
    "## 10. Test Set Evaluation with Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fba004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "if os.path.exists(NIH_TEST):\n",
    "    nih_test = SimpleMedicalDataset(NIH_TEST, DATA_ROOT, transform=get_transforms(config.img_size, False))\n",
    "    test_loader = torch.utils.data.DataLoader(nih_test, batch_size=config.batch_size, \n",
    "                                               shuffle=False, num_workers=config.num_workers)\n",
    "    \n",
    "    # Evaluate best LoRA model\n",
    "    print(\"Evaluating LoRA model on test set...\")\n",
    "    test_results = trainer_lora.test(lit_lora, dataloaders=test_loader)\n",
    "    \n",
    "    # Get predictions for bootstrap CI\n",
    "    lora_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            imgs = batch['image'].cuda()\n",
    "            labels = batch['labels'].numpy()\n",
    "            logits = lora_model(imgs)\n",
    "            preds = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    \n",
    "    # Compute metrics with CI\n",
    "    from functools import partial\n",
    "    auc_fn = partial(compute_metrics, thr=0.5)\n",
    "    ci_results = bootstrap_confidence_interval(all_preds, all_labels, \n",
    "                                                lambda p, l: compute_metrics(p, l)['auc_roc'], \n",
    "                                                n_bootstrap=1000)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST SET RESULTS (LoRA Model)\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"AUC-ROC: {ci_results['mean']:.4f} (95% CI: [{ci_results['ci_lower']:.4f}, {ci_results['ci_upper']:.4f}])\")\n",
    "    print(\"=\"*70)\n",
    "else:\n",
    "    print(\"Test set not found, skipping test evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42743952",
   "metadata": {},
   "source": [
    "## 11. Save Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf3316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv('results_comparison.csv', index=False)\n",
    "print(\"\\nâœ“ All experiments complete!\")\n",
    "print(\"âœ“ Results saved to results_comparison.csv\")\n",
    "print(\"âœ“ Checkpoints saved in ./checkpoints/\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download checkpoints and results for further analysis\")\n",
    "print(\"2. Experiment with different k values (few_shot_k)\")\n",
    "print(\"3. Try different LoRA ranks and adapter dimensions\")\n",
    "print(\"4. Compare cross-domain generalization (CheXpertâ†’NIH vs NIHâ†’CheXpert)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
